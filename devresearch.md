Perfect. I’ll explore GPU-based and hybrid frameworks, physics simulation libraries (SDF, MPM, fluid, etc.), and tools for building interactive dashboards and doc-friendly sites. I’ll also identify libraries compatible with React and high-performance rendering (WebGPU, R3F, regl) to accelerate your project’s modular blob simulation system.

I’ll let you know once I’ve gathered everything you need to build this from particle to glorious liquid blob.

# Modular Blob Simulation Engine – Architecture Proposal

## Technology Stack Overview

**Platform & Language:** Target modern web browsers using **WebGPU or WebGL2** for GPU acceleration, with a **TypeScript** codebase for type safety and maintainability. WebGPU is ideal for compute-heavy tasks (offering compute shaders for particle updates and physics) and is supported in latest browsers ([GitHub - chetan-parthiban/WebGPUMPM: An implementation of MPM running in (near) real time in the browser utilizing the new WebGPU API.](https://github.com/chetan-parthiban/WebGPUMPM#:~:text=MPM%20is%20a%20hybrid%20Eulerian%2FLagrangian,the%20midpoint%20results%20are%20realtime)). WebGL2 can serve as a fallback or interim solution, using fragment shaders and framebuffers for GPGPU computations. If extreme performance is needed, critical algorithms can be implemented in **Rust/C++** and compiled to **WebAssembly** (WASM), but many effects can be achieved in pure WebGL/WebGPU shaders.

**Rendering Engine:** Leverage a high-level rendering library for productivity and React integration. **Three.js** (with React Three Fiber) is a strong option for 3D scenes and custom shader support. Three.js provides utilities like `Points` and `InstancedMesh` for rendering thousands of particles efficiently, and its WebGL2 renderer (or experimental WebGPU renderer) can be used for shader-based physics. **Babylon.js** is another mature engine with WebGPU support, though integration with React is slightly less common. For a mostly 2D “blob” interface, one could also use lower-level frameworks like **regl** (WebGL2) for direct GPGPU control, but Three.js will simplify scene management and still allow custom GPU computations via render targets. The goal is to have rendering abstracted so we can easily visualize particles, fields, and blob surfaces in real-time.

**GPU Physics & Computation:** There isn’t an off-the-shelf engine covering all required phenomena, so the plan is to build a **modular physics engine** in-house, using GPU acceleration. We will design the simulation engine as a set of modules (particle integrator, fluid solver, field solver, etc.) that run on the GPU. For example, we can use WebGPU compute passes or WebGL framebuffers to update particle positions, interact with fields, and so on. This approach avoids expensive CPU-GPU data transfer by keeping calculations on the GPU ([glsl-fft - npm](https://www.npmjs.com/package/glsl-fft#:~:text=Is%20it%20fast%3F)). Existing physics libraries (Ammo.js, Cannon.js, Rapier) focus on rigid bodies and are CPU-based; they may be used for auxiliary features (e.g. if we need some rigid obstacle physics, Rapier WASM could be integrated), but **fluid, particle, and field dynamics will be custom GPU implementations** for performance. The engine will be written in TypeScript (with maybe GLSL or WGSL shader code strings and helper classes to manage GPU buffers). If needed, we can integrate compute kernels via WebGPU’s WGSL or use WASM for complex math (e.g. a FFT library in WASM), but the preference is to utilize **shaders for parallelism**.

**Incremental Build Approach:** We will start with a simple particle system on the GPU (single particles moving) and gradually add complexity. The modular design means early phases (like basic particles under gravity) use the same framework as later phases (fluid dynamics, etc.), just adding more shader passes or modules. Each feature (gravity, SPH fluid, reaction-diffusion, etc.) can be toggled or composed, aiding both development and debugging.

## Libraries & Tools for Simulation Features

A variety of specialized libraries and techniques will accelerate development for each aspect of the simulation:

- **GPU Particle Systems:** For managing large numbers of particles on the GPU, we can use Three.js’s utilities or lightweight libraries. Three.js’s `GPUComputationRenderer` (from its examples) provides a convenient way to update textures representing particle state each frame using fragment shaders. It has been used in many demos to achieve thousands of particles with interactive performance. Alternatively, libraries like **`three-gpu-particle-system`** (an add-on for Three.js) implement a GPU-based particle updater ([three-gpu-particle-system - GitHub](https://github.com/fazeaction/three-gpu-particle-system#:~:text=GPU%20based%20particle%20system%20for,4%20forks%20Branches%20Tags%20Activity)). These tools essentially treat particle properties as textures and update them via shaders (e.g. position and velocity textures). We can also utilize **instanced rendering** for drawing particles (one sphere or sprite instanced N times) or the built-in `Points` with a custom shader for ultra-fast point rendering. If needed, for pure computation without Three.js, **gpu.js** is a generic GPGPU library in JS, but it’s somewhat dated; using WebGL/WebGPU directly via shader code will be more efficient.

- **Gravity and N-Body Interaction:** A simple module can handle gravitational attraction between particles or celestial-body style motion. This can be implemented with an $O(n^2)$ shader for small n or using barnes-hut approximation for larger n. There’s no specific library required beyond writing a compute shader or fragment shader for the force accumulation. Examples from GPU flocking or galaxy simulations can be followed. Since this is an early stepping stone, a basic integrator (e.g. velocity Verlet) in a shader will suffice. We mention this for completeness even if not explicitly requested as a separate library – it will be part of our engine’s custom code.

- **Fluid Dynamics (SPH/PIC/FLIP):** For fluid simulation, there are a few approaches:
  - *Grid-based Eulerian fluid:* We can implement a WebGL2-based solver of the Navier-Stokes equations (Stable Fluids by Stam). In fact, open-source WebGL implementations exist – e.g. Pavel Dobryakov’s **WebGL Fluid Simulation** is a 2D Navier-Stokes demo that runs in realtime in browsers (including mobile) ([fluid-simulation · GitHub Topics · GitHub](https://github.com/topics/fluid-simulation?l=typescript#:~:text=WebGL%20Fluid%20Simulation%20for%20modern,works%20even%20on%20mobile)). We could reuse or adapt such code, which uses a ping-pong FBO technique to update velocity and density fields. There’s also **navier-stokes-webgl** (JavaScript/TS) for stable fluid simulation ([fluid-simulation · GitHub Topics · GitHub](https://github.com/topics/fluid-simulation?l=typescript#:~:text=Stable%20fluid%20simulation%20on%20GPU,using%20WebGL)) that could be referenced or forked. These provide a starting point for fluid advection, diffusion, and pressure solve on a grid.
  - *Particle-based fluid (SPH):* Smoothed Particle Hydrodynamics could be implemented for a more “blob”-like fluid with surface tension. While no complete JS library is mainstream, we can build on research code. For instance, **LiquidFun** (a fork of Box2D) adds particle-based fluid simulation; **liquidfun-wasm** is a revived version available as WebAssembly with TypeScript bindings ([Show HN: WASM and WebGL Fluid Simulation | Hacker News](https://news.ycombinator.com/item?id=27996703#:~:text=Author%20here,body%20physics%20to%20Box2D%5B2)). This can simulate 2D fluid and soft bodies with decent performance. We might use liquidfun-wasm for 2D demos or as a comparison baseline. For 3D SPH, one might look at Unity or C++ implementations and port critical parts to shader code.
  - *Material Point Method (MPM):* MPM is particularly powerful for **continuum simulation** (fluids, gooey materials, sand, etc.) and is well-suited to GPU parallelization. Recent open projects like **PB-MPM** (Position-Based MPM) from EA show that WebGPU can handle 200k particles in real time ([GitHub - chetan-parthiban/WebGPUMPM: An implementation of MPM running in (near) real time in the browser utilizing the new WebGPU API.](https://github.com/chetan-parthiban/WebGPUMPM#:~:text=MPM%20is%20a%20hybrid%20Eulerian%2FLagrangian,the%20midpoint%20results%20are%20realtime)), covering fluids, jelly-like solids, and snow. We can draw inspiration from that codebase (published for SIGGRAPH 2024) for our engine’s architecture. MPM represents materials as particles with an underlying grid for computations ([Material Point Method](https://windqaq.github.io/MPM/#:~:text=The%20material%20point%20method%20,such%20as%20the%20deformation%20gradient)). Implementing MPM from scratch is complex, but we could start with simpler PIC/FLIP (Particle-in-Cell) for fluids and gradually incorporate MPM features (like deformation gradients for solid behavior). If the timeline allows, we may even integrate or wrap the EA PB-MPM WebGPU code as a module in our engine – it’s open source and could save time, though it might need simplification for our use-case (their code is optimized and somewhat hard to read, per their note).
  - *Moving Least Squares (MLS):* In the context of fluid/MPM, MLS can refer to **MLS-MPM**, an improvement to MPM’s grid projection step to reduce numerical dissipation. We will consider implementing MLS-MPM for better accuracy in mixing fluids or simulating granular detail. Another use of MLS is for smoothing particle positions or constructing surfaces from particles. We might utilize MLS for rendering – e.g., to compute a smooth surface mesh or implicit field from particle data. There isn’t an out-of-box library for MLS in JS; it will be part of our custom solver module if used.
  
- **Signed Distance Fields (SDF) & Field Representations:** Signed distance fields will be useful for representing obstacle boundaries, blob surfaces, and collision domains in a resolution-independent way. We can incorporate SDFs in two ways: 
  1. *Analytical SDFs:* For simple shapes (planes, spheres, etc.), we can code the distance functions directly in shaders (using known formulas ([3D SDFs - Inigo Quilez](https://iquilezles.org/articles/distfunctions/#:~:text=3D%20SDFs%20,as%20well%20as%20some)) from resources like Inigo Quilez’s library of distance functions). This allows collisions with primitive shapes by just evaluating a formula for distance and applying a repulsive force or boundary condition when distance < 0.
  2. *Grid SDFs:* For arbitrary or dynamic shapes, we can generate SDFs on the fly. A great tool here is **gpu-distance-field** (TS library) which uses the Jump Flooding algorithm to compute SDF from images on the GPU ([GitHub - ryankaplan/gpu-distance-field: Typescript library for generating distance fields from 2d images on the GPU](https://github.com/ryankaplan/gpu-distance-field#:~:text=gpu)). This library can produce an SDF texture each frame fast enough for 60 FPS updates, which is valuable if our “blob” shapes are evolving and we need their distance field for collisions or rendering. We might use this library to compute SDF of fluid particle splats or of any user-drawn obstacles in the scene. The output SDF can then be sampled in shaders for collision response (particles checking distance to boundary) or for rendering iso-surfaces (like rendering the zero level-set as a contour).
  
  Additionally, SDFs tie into rendering: We can raymarch the SDF for a smooth iso-surface visualization of blobs merging, or use marching cubes (Three.js has a MarchingCubes example for iso-surfaces) to extract a mesh each frame from a 3D SDF field. For 2D visuals, marching *squares* or just drawing the field as a texture is sufficient. The SDF approach ensures blob-like smooth shapes when particles cluster, achieving the “metaball” look without manually managing meshes.

- **Fourier Transforms & Diffusion:** Real-time Fourier transforms might be needed for certain effects – for example, spectral fluid solvers, wave simulations, or doing fast convolution for diffusion. If we require FFT on the GPU (e.g., to solve a Poisson equation or to generate wave spectra for an ocean surface), we can use libraries like **glsl-fft**. This library provides a GLSL shader implementation of 2D FFT and is designed to work with frameworks like regl ([glsl-fft - npm](https://www.npmjs.com/package/glsl-fft#:~:text=What%20does%20it%20compute%3F)) ([glsl-fft - npm](https://www.npmjs.com/package/glsl-fft#:~:text=Is%20it%20fast%3F)). While not heavily optimized, it is faster than transferring data back to CPU for an FFT ([glsl-fft - npm](https://www.npmjs.com/package/glsl-fft#:~:text=Is%20it%20fast%3F)). We can integrate glsl-fft in our engine’s pipeline if we need to compute frequency-domain operations (such as filtering a field or computing a fast diffusion). Another route is to use **TensorFlow.js** FFT (if we include tfjs GPU backend), but that might add a large dependency; glsl-fft is lightweight in comparison. For 1D FFT (maybe for audio reactive features or analysis), we could also use the WebAudio API’s FFT for sound input, but that’s outside the core simulation’s scope.

  **Diffusion Fields:** For pure diffusion of heat, dye, or other scalar fields, an alternative to FFT is using iterative solvers. We can simply implement a Jacobi or Gauss-Seidel iteration in a shader to diffuse a field, which is often what 2D fluid solvers do for the diffusion step. Since we aim for real-time, a modest number of iterations or an explicit time step might be enough if the time step is small. Another advanced technique is using **Moving Least Squares (MLS)** to smoothly interpolate values at particle positions for diffusion of quantities carried by particles (useful in MPM or SPH for temperature or color diffusion). This would be part of the particle solver rather than a separate library.

- **Reaction-Diffusion & Chemical Patterns:** Reaction-diffusion systems (like Gray-Scott model) can create the kind of organic **patterns on blobs** that the user is interested in. We plan to incorporate a module for reaction-diffusion which operates on a grid (texture) and can interact with the fluid or particles. A simple implementation of Gray-Scott two-chemical RD is straightforward with a fragment shader ping-ponging between two textures (for chemical concentrations) each frame. There are existing projects we can draw from; e.g. **reaction-diffusion-webgl** by piellardj provides a GPU implementation of the Gray-Scott model producing coral-like or leopard-spot patterns ([GitHub - piellardj/reaction-diffusion-webgl: Reaction-diffusion on GPU in WebGL.](https://github.com/piellardj/reaction-diffusion-webgl#:~:text=Reaction,of%20%27B%27%20slowly%20destroys%20itself)). We can reuse formulas and even code from such projects to accelerate development. The RD module will allow us to overlay chemical dynamics either on a static grid or moving with the fluid flow (by coupling the advection of chemicals to the fluid velocity field). This could simulate, for example, chemicals reacting in a fluid or pigmentation patterns forming on moving blobs.

  **Collision Handling & Hydrophobic Effects:** Collision detection in our engine will likely use a mix of SDF-based detection (for continuous fields and boundaries) and particle-based checks (for particle-particle interactions). For example, fluid particles can collide with solid boundaries by checking an SDF for penetration depth and applying a corrective impulse. For particle-particle collisions in a granular context, we could use spatial hashing or uniform grid to detect neighbors (common in SPH). Given performance constraints, purely GPU-driven broadphase (like using a grid or sort) is preferable to CPU-based checking. If needed, a lightweight physics library like **Box2D/LiquidFun** (via liquidfun-wasm) could handle coarse collision resolution in 2D, but for 3D we will implement custom routines or possibly use **Rapier** (a Rust physics engine compiled to WASM) for rigid body interactions if we introduce any rigid objects.

  *Hydrophobic interactions* specifically refer to how fluid interacts with surfaces (e.g. water beading up on a hydrophobic surface). To simulate this, our fluid solver must support **surface tension** and adhesion parameters. We’ll ensure the fluid model (whether SPH or grid-based) has a surface tension term. For instance, SPH has cohesion forces that can be tuned. We can treat hydrophobic surfaces by giving them a contact angle – effectively adjusting how fluid particles or the level-set interface behaves near the boundary. In practice, this could mean applying a slight repulsive force at the contact line or modifying boundary conditions so fluid doesn’t wet the surface. We found an example of a fluid sim demo that includes surface tension, viscosity, and even bubble dynamics ([Water | Hacker News](https://news.ycombinator.com/item?id=37026592#:~:text=A%20web%202D%20fluid%20simulation,air%20bubbles%20is%20this%20one)), indicating these effects are attainable. Our engine will incorporate a parameter for surface tension (to create droplets) and allow “coating” certain objects or regions with hydrophobic property (meaning low adhesion). No standalone library does this out of the box, but the physics are achievable by extending the fluid solver – for example, by adding a cohesive force between fluid particles that is strong (causing beading) and by ensuring boundary normals cause fluid to recoil if approaching a hydrophobic surface. Reaction-diffusion on a moving fluid can also simulate chemical gradients that affect surface tension (the Marangoni effect), which could lead to interesting “hydrophobic” pattern dynamics – these are advanced possibilities if time permits.

 ([GitHub - piellardj/reaction-diffusion-webgl: Reaction-diffusion on GPU in WebGL.](https://github.com/piellardj/reaction-diffusion-webgl)) *Example of a reaction-diffusion pattern forming a stylized Mona Lisa, generated by a Gray-Scott GPU simulation. The system continuously converts chemical A to B and diffuses both, creating complex spot and stripe patterns.* Reaction-diffusion modules like this will be integrated into the engine, so users can spawn such patterns on fluid surfaces or within blobs. The image above (from an open-source WebGL demo) demonstrates the **visually rich** outcomes possible with GPU computation and will serve as inspiration for the aesthetic goals ([GitHub - piellardj/reaction-diffusion-webgl: Reaction-diffusion on GPU in WebGL.](https://github.com/piellardj/reaction-diffusion-webgl#:~:text=Reaction,of%20%27B%27%20slowly%20destroys%20itself)).

- **Field Visualization:** In addition to simulation, we should consider libraries for visualizing vector fields or scalar fields. If we want to show velocity fields, we could use **arrow helpers** in Three.js or a custom shader to draw line integral convolution or particles moving with the flow. There are no specific external libraries required, but techniques like line integral convolution (LIC) could be implemented via shader if needed to depict flow. For scalar fields (pressure, chemical concentration), using colormapped textures on a quad or isosurfaces via marching cubes (Three.js has `MarchingCubes` in examples) are good approaches. These will be custom visualization components built on top of the rendering engine.

## React Dashboard & Interaction Layer

For the user interface and interactivity, we will build a **React** application that serves as a dashboard to control the simulation and visualize results in real-time. Key libraries and components for the dashboard include:

- **React Three Fiber (R3F):** This is a React renderer for Three.js, which allows us to declaratively describe 3D scenes in JSX. Using R3F will make it easier to integrate the WebGL canvas into React, handle resizing, and overlay HTML UI if needed. We can create a `<Canvas>` component (from R3F) that hosts our simulation scene. R3F also has a thriving ecosystem (drei) with helpers that could be useful (orbit controls, stats panel, etc.). While R3F is not strictly required, it helps **React compatibility** immensely by making the 3D scene part of the React component tree. If we decide to manage the WebGL context manually, we’d have to interface with React via `useEffect` and refs, which is doable but less elegant.

- **State Management (Zustand or Recoil):** To manage simulation parameters and application state, we will use a lightweight state manager. **Zustand** is a great choice – it’s a small, fast state-management library with an intuitive hooks API ([Zustand: Introduction](https://zustand.docs.pmnd.rs/#:~:text=A%20small%2C%20fast%2C%20and%20scalable,It%20isn%27t%20boilerplatey%20or%20opinionated)). Zustand can hold global state (like current simulation settings, toggles for modules, UI state) that both the React components and even non-React code (e.g. the simulation loop) can access. Its unopinionated nature means we avoid boilerplate and can easily connect UI controls to simulation internals. Another option is **Recoil** (especially if we want to manage state as atoms/selectors), but Recoil might be overkill here. Zustand’s simplicity (“bearbones” flux-like store ([Zustand: Introduction](https://zustand.docs.pmnd.rs/#:~:text=A%20small%2C%20fast%2C%20and%20scalable,It%20isn%27t%20boilerplatey%20or%20opinionated))) fits our needs for controlling parameters like particle count, gravity on/off, fluid viscosity, etc. from the React side.

- **Parameter Controls UI (Leva or alternatives):** For a smooth interactive experience, we’ll use a GUI library to create a control panel for simulation parameters. **Leva** is a React-first GUI library that provides a convenient, beautiful panel for adjusting values at runtime ([Introducing Leva, a GUI made for React : r/reactjs - Reddit](https://www.reddit.com/r/reactjs/comments/lv5rrs/introducing_leva_a_gui_made_for_react/#:~:text=Introducing%20Leva%2C%20a%20GUI%20made,useState%20but%20with%20a%20GUI)). With Leva, we can expose sliders, color pickers, toggles, and numeric inputs for any state in our zustand store or component state – it’s like a modern replacement for dat.GUI, designed for React. For example, we can have sliders for “Viscosity”, “Gravity Strength”, “Particle Count” and see their effect immediately in the simulation. Leva integrates nicely with R3F (tutorials show how to use it to tweak scene properties ([Leva - React Three Fiber Tutorials](https://sbcode.net/react-three-fiber/leva/#:~:text=Leva%20,npm%20install%20leva))) and will be instrumental for debugging and demonstrating different scenarios. An alternative is **Tweakpane** or **Theatre.js** (which is more for keyframing animations), but Leva should suffice and is lightweight.

- **Component Library / Styling:** We will use standard React component libraries as needed for layout. For instance, if we need a sidebar or tabs, we might use Chakra UI or Material UI, but given the focus, a minimal custom UI might do. The dashboard will mostly consist of the simulation canvas and the Leva controls, maybe some info display (FPS, particle count) and possibly a timeline or play/pause button. React’s ecosystem provides easy additions like a **FPS Stats** component (there’s `drei/Stats` or we can embed stats.js) to monitor performance.

- **Storybook for UI development:** We will set up **Storybook** as a development tool for the React components (especially the UI controls and any visualization components) ([Why Storybook? | Storybook docs](https://storybook.js.org/docs/get-started/why-storybook#:~:text=Why%20Storybook%3F%20,development%2C%20testing%2C%20and%20documentation)). Storybook allows us to build and test UI components in isolation, which is useful for the dashboard elements. For example, we can create a Storybook story for the control panel that simulates various states, or a story for a particle canvas with dummy data to ensure rendering works on different screen sizes. Storybook is essentially a “frontend workshop” environment ([Why Storybook? | Storybook docs](https://storybook.js.org/docs/get-started/why-storybook#:~:text=Why%20Storybook%3F%20,development%2C%20testing%2C%20and%20documentation)) – it can also serve as living documentation for our UI. This is optional but recommended for a polished outcome, and it can be integrated into the monorepo (likely as a dev dependency with stories co-located with components).

- **Global State Synchronization:** The React app will need to start/stop and feed parameters to the simulation engine. We’ll implement a small layer to do this: for example, a custom React hook that wraps the simulation engine. If the engine runs in the same thread, this might simply expose functions like `initSimulation(canvas)`, `updateParams(params)` and `togglePause()`. If we offload simulation to a WebWorker or OffscreenCanvas (to not block the UI), we would have to communicate via postMessage. Initially, we can run everything on the main thread using requestAnimationFrame in the React render loop or a custom game loop. As performance demands grow, we can explore moving the heavy logic to a worker (especially if using OffscreenCanvas with WebGL or the new ability of WebGPU to run in workers). Regardless, the React controls will dispatch actions to update the simulation. Zustand store can help here by allowing the simulation code to read latest parameters each frame without prop-drilling through many components.

- **Event Handling & Interaction:** The interface will support interactive input – e.g. clicking or touching the canvas to add particles or “stir” the fluid. We can use R3F’s pointer events or raw DOM events on the canvas to capture these and then translate into simulation commands (like inject dye or spawn a blob). This interactive aspect will make the system engaging. Libraries like **React Use Gesture** could be helpful for interpreting drag gestures (for example, dragging a particle emitter around). For simplicity, we might start with basic mouse events.

## Documentation & Learning Site

We will include a comprehensive **documentation and learning website** as part of the project, so that users and contributors can understand and experiment with the engine. For this purpose, a static-site generator with good support for technical content is ideal:

- **Docusaurus 2** (by Meta) is a top choice for documentation. It’s a React-based static site generator optimized for documentation sites, with built-in versioning, search, and theming ([Docusaurus: Build optimized websites quickly, focus on your content](https://docusaurus.io/#:~:text=Docusaurus%3A%20Build%20optimized%20websites%20quickly%2C,blogs%2C%20marketing%20pages%2C%20and%20more)). We can quickly set up a Docusaurus site for our project, with sections for **Getting Started**, **Module Guides (Particles, Fluid, etc.)**, and an interactive demo gallery. Docusaurus would allow embedding React components in Markdown (using MDX), meaning we could even embed live simulation examples or UI elements directly in the docs. The site will be deployed to GitHub Pages or a similar static host via CI. Docusaurus’s default theme includes a sidebar for navigation and a top menu, which will nicely organize our content.

- **Alternatives:** If for some reason we prefer not to use React for docs (though given we are using React anyway, Docusaurus fits well), we could consider **VitePress** or **VuePress** (which are Vue-powered). VitePress is lean and fast, focused on Markdown-driven docs with minimal config ([How to Build a Modern Documentation Site with VitePress](https://www.freecodecamp.org/news/how-to-build-a-modern-documentation-site-with-vitepress/#:~:text=How%20to%20Build%20a%20Modern,It%20is%20powered)). It’s great for documentation but might complicate embedding our React components (though it’s possible via iframe or by using a minimal build of our canvas as a web component). **Astro** is another modern choice – Astro can integrate React, Svelte, etc., and would let us build a content-rich site with perhaps less overhead than Docusaurus. Astro is more of a general web framework for content sites ([Astro](https://astro.build/#:~:text=Astro%20is%20a%20JavaScript%20web,website%20performance%20by%20rendering)); we could use Astro if we wanted a custom-styled site that pulls in dynamic demos. However, the speed of setup and rich features of Docusaurus (search, versioning, nice default theme) likely make it the best choice. 

In summary, we propose **Docusaurus** for the docs site, as it will allow us to **“focus on content”** and easily maintain API docs and tutorials ([Docusaurus: Build optimized websites quickly, focus on your content](https://docusaurus.io/#:~:text=Docusaurus%3A%20Build%20optimized%20websites%20quickly%2C,blogs%2C%20marketing%20pages%2C%20and%20more)). The docs will live in the monorepo (e.g. in a `docs/` directory or as a separate package). We will write guides for each subsystem (explaining, for instance, how the fluid solver works, with equations and maybe embedded videos or images), and we can also have a section for developers on how to extend the engine. The site can host **live demos** using iframes – for example, we can build a special demo bundle of the dashboard and embed it in the docs to let visitors play with the simulation parameters in-page.

Additionally, we can maintain a **Storybook** as mentioned for UI, and even publish the storybook as a static site for documentation of the UI components if needed. But the primary learning materials and technical explanations will be in the Docusaurus site.

## Project Structure and Monorepo Setup

We will use a **monorepo** approach to keep the simulation engine, the dashboard app, and the documentation all in one repository. This makes it easy to keep versions in sync and allows sharing of code (for example, maybe some utility functions or types) between packages. The suggested structure is:

```
repo-root/
├── packages/
│   ├── simulation-engine/    # Core engine library (TypeScript, containing GPU shaders, logic)
│   ├── dashboard-app/        # React application for UI and visualization
│   └── docs-site/            # Documentation site (Docusaurus or others)
├── package.json             # Workspace root definitions
├── tsconfig.json            # Base TS config
├── etc.
```

We will use **Yarn Workspaces** or **PNPM Workspaces** to manage the monorepo (ensuring all packages are part of one dependency graph). This allows running a single install for all and easy cross-references (the dashboard can import the engine as a package). For build tools, we can introduce **Turborepo** or **Nx** if needed to orchestrate builds and caching, but initially a simpler setup with package scripts might suffice. Each package will have its own `package.json` and build scripts:

- **simulation-engine:** This will compile to an npm package (even if it’s mostly internal). We’ll bundle it as an ESM module so that it can be imported by the dashboard. We might use **Rollup** or **esbuild/tsup** to bundle the engine, especially if it includes GLSL shader files (which we may import as strings or use a loader). The engine package should be framework-agnostic – no React imports here, just pure logic. This could also output type declarations for others to use. If the engine grows, we could further split it (e.g. engine-core, engine-react-bindings), but likely not needed at first.

- **dashboard-app:** This is a React app (could be a single-page app). We can use **Vite** for rapid development (Vite is fast and easy to configure for TS + React + Three.js). Alternatively, Next.js could be used if we wanted server-side rendering, but that’s not necessary for an internal dashboard. The dashboard will import the engine as a dependency. We will ensure hot-reloading works across the monorepo (Vite can watch workspace packages). The app will have its own dev server for local development of the simulation UI.

- **docs-site:** If using Docusaurus, it comes with its own CLI (`docusaurus start`). We will treat it as a separate website. We can integrate it with the monorepo by allowing it to import some parts of the engine code or data if needed (for instance, we could write MDX that imports a JSON of default config from the engine). The docs site will be built and deployed via CI. 

- **Storybook:** We can configure Storybook inside the dashboard package (since the UI components live there). It can be launched with `yarn storybook` and will pick up stories from the `dashboard-app` components. We will ensure the storybook is also aware of the engine package (which should be fine if linked via workspace).

This monorepo setup yields a clear separation of concerns: the core simulation logic vs. the UI vs. the learning materials. It also makes it possible for others to use the simulation engine independently of our app (they could import `simulation-engine` in their own project if we publish it). We’ll maintain consistency by using a shared ESLint and Prettier config at the root, and maybe a tool like **Changesets** if we version/publish packages.

**Build and Performance:** We prioritize performance at every step. The engine will be built with optimizations (minimization for production). For development, we keep things fast with hot reload. We will also include profiling tools – for example, using the Performance API in the engine to measure frame times, and perhaps a UI display of milliseconds per physics frame. Since the heavy lifting is on the GPU, the CPU load should remain low, but we will monitor things like memory usage (large GPU buffer allocations, etc.). The React app will be kept lightweight (mostly just controlling the simulation and not doing heavy computations in JS). By using WebGL/WebGPU directly, we ensure the browser’s highly optimized graphics pipeline is utilized fully.

## Reusing and Forking Open-Source Projects

To accelerate development, we plan to reuse code and ideas from several open-source projects (all compatible with permissive licenses):

- **Liquidfun-wasm** – Provides a ready-made 2D particle-based fluid and soft body simulation ([Show HN: WASM and WebGL Fluid Simulation | Hacker News](https://news.ycombinator.com/item?id=27996703#:~:text=Author%20here,body%20physics%20to%20Box2D%5B2)). We can use this for reference or even as a fallback for 2D mode. Its integration of Box2D with fluid particles can guide our collision handling for particles. If nothing else, it serves as a performance reference point for particle simulations in WASM vs. our GPU approach.

- **WebGL Fluid Simulation (Dobryakov et al.)** – We will study the shader code from this project (and its forks like WebGL-Fluid-Enhanced). It has well-implemented advection, diffusion, and divergence-free projection steps for fluid. Incorporating those shaders (which are typically under MIT license) into our engine could save time on the fluid solver. It’s also known to run on mobile at decent speed ([fluid-simulation · GitHub Topics · GitHub](https://github.com/topics/fluid-simulation?l=typescript#:~:text=WebGL%20Fluid%20Simulation%20for%20modern,works%20even%20on%20mobile)), which is a good sign for efficiency.

- **WebGPU MPM Demos** – Both the Electronic Arts PB-MPM ([GitHub - electronicarts/pbmpm: A WebGPU implementation of Position Based MPM (PB-MPM), presented at SIGGRAPH 2024.](https://github.com/electronicarts/pbmpm#:~:text=This%20package%20is%20a%20WebGPU,MPM)) and Chetan Parthiban’s WebGPUMPM ([GitHub - chetan-parthiban/WebGPUMPM: An implementation of MPM running in (near) real time in the browser utilizing the new WebGPU API.](https://github.com/chetan-parthiban/WebGPUMPM#:~:text=MPM%20is%20a%20hybrid%20Eulerian%2FLagrangian,the%20midpoint%20results%20are%20realtime)) are gold mines for understanding how to structure an MPM solver on the GPU. We might not directly copy the code, but we will use their approaches (like data structures for particles and grids in GPU memory, work group strategies, etc.). If licensing allows, we could even fork PB-MPM and trim it down to our needs (perhaps focusing on the fluid part of it). At minimum, we can incorporate their learnings on optimization.

- **Reaction-Diffusion WebGL** – We will reuse the Gray-Scott shader logic from piellardj’s project ([GitHub - piellardj/reaction-diffusion-webgl: Reaction-diffusion on GPU in WebGL.](https://github.com/piellardj/reaction-diffusion-webgl#:~:text=Reaction,of%20%27B%27%20slowly%20destroys%20itself)) or others. Reaction-diffusion isn’t trivial to derive from scratch without error; using proven formulas will help. Additionally, we might incorporate different reaction models (there are other patterns like FitzHugh-Nagumo, etc.) by looking at open source implementations (some Processing or GPU Gems examples are available).

- **Jump Flood SDF (gpu-distance-field)** – We’ll integrate this library directly for dynamic SDF generation ([GitHub - ryankaplan/gpu-distance-field: Typescript library for generating distance fields from 2d images on the GPU](https://github.com/ryankaplan/gpu-distance-field#:~:text=gpu)). It has an MIT license and no dependencies, so it should slot into our engine nicely. This saves us from writing our own jump flooding algorithm for SDF (which is doable, but nice to have pre-made).

- **UI Libraries** – Leva and Zustand are open source (MIT) and we will use them rather than reinventing a GUI or state logic. Storybook is also open source and will be used as-is.

- **Three.js** – Obviously, we rely on Three.js for rendering. Its shaders (like for postprocessing or particle materials) could be leveraged for effects (bloom, etc. if we want to add post-processing to make the visuals pop).

- **APHROS (CSE-Lab)** – The comment we found ([Water | Hacker News](https://news.ycombinator.com/item?id=37026592#:~:text=A%20web%202D%20fluid%20simulation,air%20bubbles%20is%20this%20one)) references a demo with surface tension and bubbles. We should see if CSELabs’ **Aphros** project (often a C++ CFD code) has a web/WASM version or source. If available, it might give insights into handling multi-phase fluids or electrohydrodynamics. Even if we don’t use their code, studying it could inform our hydrophobic interaction module.

- **Documentation tools** – We will use Docusaurus’s standard templates but customize content. Perhaps we can also use MDX embeds to show our Storybook stories or to include live CodeSandbox-like editors for small examples (Docusaurus has a live code block component for React).

By standing on the shoulders of these projects, we ensure we’re using **best-in-class algorithms**. Wherever we reuse code, we’ll attribute appropriately in documentation. The end result will be a fusion of these techniques into a **unified, modular system**.

---

## Conclusion

In summary, this architecture leverages a **powerful GPU-accelerated core** with WebGPU/WebGL, a **flexible React-based UI** for interactivity, and a **comprehensive docs site** – all tied together in a maintainable monorepo. The engine will be modular, allowing features like particle gravity, fluid dynamics, SDF collisions, reaction-diffusion, and even advanced MPM to be enabled or disabled as needed. Performance is prioritized by doing heavy computation on the GPU and minimizing thread overhead, aiming for real-time feedback even with complex phenomena. The technology stack (TypeScript, Three.js/R3F, shaders, and modern state management) ensures both developer productivity and compatibility with web platforms.

This proposal provides a roadmap to move from simple particles to full “blob” simulations with rich visuals. By following it, the user can incrementally build the system, testing each part (thanks to the React dashboard and Storybook) and educating others (via the Docusaurus site) along the way. The end goal is an interactive simulation sandbox that is not only high-performance and beautiful, but also **extensible** for future research or creative experimentation. With the suggested tools and libraries, we maximize reuse of existing work and focus our effort on the novel integration of these elements into a cohesive blob simulation engine.

**Sources:** The plan references established projects and libraries to justify choices, for example real-time WebGPU particle demos ([GitHub - chetan-parthiban/WebGPUMPM: An implementation of MPM running in (near) real time in the browser utilizing the new WebGPU API.](https://github.com/chetan-parthiban/WebGPUMPM#:~:text=MPM%20is%20a%20hybrid%20Eulerian%2FLagrangian,the%20midpoint%20results%20are%20realtime)), WebAssembly physics engines ([Show HN: WASM and WebGL Fluid Simulation | Hacker News](https://news.ycombinator.com/item?id=27996703#:~:text=Author%20here,body%20physics%20to%20Box2D%5B2)), GPU fluid and reaction-diffusion examples ([GitHub - piellardj/reaction-diffusion-webgl: Reaction-diffusion on GPU in WebGL.](https://github.com/piellardj/reaction-diffusion-webgl#:~:text=Reaction,of%20%27B%27%20slowly%20destroys%20itself)) ([fluid-simulation · GitHub Topics · GitHub](https://github.com/topics/fluid-simulation?l=typescript#:~:text=WebGL%20Fluid%20Simulation%20for%20modern,works%20even%20on%20mobile)), and modern UI tools for React ([Introducing Leva, a GUI made for React : r/reactjs - Reddit](https://www.reddit.com/r/reactjs/comments/lv5rrs/introducing_leva_a_gui_made_for_react/#:~:text=Introducing%20Leva%2C%20a%20GUI%20made,useState%20but%20with%20a%20GUI)) ([Zustand: Introduction](https://zustand.docs.pmnd.rs/#:~:text=A%20small%2C%20fast%2C%20and%20scalable,It%20isn%27t%20boilerplatey%20or%20opinionated)) ([Why Storybook? | Storybook docs](https://storybook.js.org/docs/get-started/why-storybook#:~:text=Why%20Storybook%3F%20,development%2C%20testing%2C%20and%20documentation)). These sources demonstrate the feasibility of each component and guide our implementation strategy. The result will be a state-of-the-art simulation platform built on proven technologies and algorithms.